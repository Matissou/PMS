---
title: "Partiel_2017"
author: "Matisse Landais"
date: "26 octobre 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Q1 : Histogramme
On commence par construire 
```{r}
donnees <- c(-0.97,-0.96,-0.93,-0.91,-0.90,-0.87,-0.85,-0.80,-0.75,-0.73,-0.66,-0.65,-0.63,-0.60,-0.58,-0.51,-0.48,-0.45,-0.44,-0.40,-0.37,-0.33,-0.3,-0.25,-0.12,-0.08,0.25,0.41,0.51,0.68)
```



```{r}
histogrammeClasse <-function(tab) {
  # Soit c le tableau que l'on veut plot.
  
  k = 0 # Nombre de classes
  a0 = 0 # Borne min
  ak = 0 # Borne max
  h = 0 # Hauteur du rectangle
  
  # Calcul du nombre de classes
  if (length(tab) <= 22) {
    k = 5 
  }else{
    k = 1 + round(log2(length(tab)))
  }
  
  # Choix des bornes
  # Premier et dernier élément du table
  preElem = tab[1]
  derElem = tail(tab,n=1)
  
  # Premiere borne et dernière borne. 0 et 260 dans notre cas (ampoule)
  a0 = preElem - 0.025*(derElem - preElem)
  ak = derElem + 0.025*(derElem - preElem)
  
  # Calcul de la largeur 
  h = (ak - a0)/k
  
  br <- seq(-1.5,1.5,0.5)
  # On plot
  colors = c("red", "yellow", "green", "violet", "orange", "blue", "pink", "cyan")
  hist = hist(tab, prob=T, col=colors,breaks=br,xlim=c(-1.5,1.5))
  lines(hist$mids, hist$density, lwd=3, col="Red")
  lines(density(tab), lwd=3, col="Blue")
}

```


```{r, echo=FALSE}

limAbs <- 1
limOrd <- 0.009
histogrammeClasse(donnees)
```


## Q2
```{r}
graphProba <- function(xi,hxi,limy)
{
  plot(xi,hxi)
}

# x représente le tableau non trié avec les observations sur les ampoules
# log(1 - seq(1:9)/10) est la transformation de la fonction de répartition de la loi exponentielle
# sort(x) [1:9] est le tableau x trié en ne gardant que les 9 premiers éléments
lastElem <- length(donnees)
blElem <- length(donnees)-1
#limy<-c(-2.5,0.1)
myxi <- sort(donnees)[1:lastElem]
myhxi <- (sqrt(1-(seq(1:lastElem)/lastElem)))

graphProba(myxi,myhxi)
``` 
Les points sont alignés, la loi triangle semble convenir. 


```{r}
# ecdf : fonction de répartition dans laquelle on met les données
fe <- ecdf(donnees)
plot(donnees, sqrt(1-fe(donnees)), main="sqrt(1-fe(x))")
```

## Q3 
```{r}

# Simulation 1 : avec la loi triangle T
simTriangle <- function (t) {
  # On utilise T pour simuler la loi triangle
  return(t-((2*t)*sqrt(1-runif(1))))
}

# Simulation 2 : avec l'inversion de la fdr de X sur U
Fx <- function(x) {
  # Fonction de répartition
  return(1-(((tt-x)^2)/(4*tt^2)))
}

inverse = function (f, lower = -100, upper = 100) {
  # F-1(U) suit la loi de X
  function (y) uniroot((function (x) f(x) - y), lower = lower, upper = upper)[1]
}
main2 <- function() {
  tt <<- 4
  simTriInv <- inverse(Fx, -tt, tt)
  n=3000
  vect=numeric(n)
  for(i in 1:n) {
    # On simule la loi triangle on prenant l'inverse de la fonction de répartition de T et en l'appliquant sur U, une loi uniforme
    vect[i] <- unlist(simTriInv(runif(1))[1], use.names=FALSE)
  }
  
  hist(vect, col=rgb(1,0,0,0.5))
  
  vect=numeric(n)
  for(i in 1:n) {
    vect[i] = simTriangle(4)
  }
  # En utilisant la variable T 
  hist(vect, col=rgb(0,1,0,0.5), add=T)
}
main2()
```


## Q4 
Le premier propose de faire -3 fois le jeu de données 
```{r}
simTriN <- function(n, t) {
  vect=numeric(n)
  for(i in 1:n) {
    vect[i] = simTriangle(t)
  }
  return(vect)
}

main3 <- function(nIt) {
  # On prend nIt fois des samples de 30 
  # On prend -3 fois la moyenne de ce sample et on l'ajoute à tSim
  # On fait tSim/nIt pour obtenir la moyenne. On fait : abs(tSim-t) pour savoir notre biais empirique 
  tSim <- 0
  tSim2 <- 0
  tSim3 <- 0 
  tSim4 <- 0 
  t <- 4
  scoreDispersion <- 0
  scoreDispersion2 <- 0
  scoreDispersion3 <- 0 
  scoreDispersion4 <- 0
  for(i in 1:nIt){
    if(nIt == 1)
    {
      vect <- simTriN(3000, t)
    }else{
      vect <- simTriN(30, t)
    }
    
    tSim <- -3*mean(vect) + tSim
    tSim2 <- max(vect) + tSim2
    tSim3 <- -1*min(vect) + tSim3
    tSim4 <- (max(vect)-min(vect))/2  +tSim4
    scoreDispersion <- scoreDispersion + abs(tSim-t)
    scoreDispersion2 <- scoreDispersion2 + abs(tSim2-t)
    scoreDispersion3 <- scoreDispersion3 + abs(tSim3-t)
    scoreDispersion4 <- scoreDispersion4 + abs(tSim4-t)
  }
  tSim <- tSim/nIt
  cat(tSim)
  cat("\nbiais = ", ((tSim/t)*100)-100, "%")
  cat("\nScore dispersion = ", scoreDispersion)
  cat("\n")
  tSim2 <- tSim2/nIt
  cat(tSim)
  cat("\nbiais methode 2 = ", ((tSim2/t)*100)-100, "%")
  cat("\nScore dispersion = ", scoreDispersion2)
  cat("\n")
  tSim3 <- tSim3/nIt
  cat(tSim)
  cat("\nbiais methode 3 = ", ((tSim3/t)*100)-100, "%")
  cat("\nScore dispersion = ", scoreDispersion3)
  
  
  cat("\n")
  tSim4 <- tSim4/nIt
  cat(tSim)
  cat("\nbiais methode 4 = ", ((tSim4/t)*100)-100, "%")
  cat("\nScore dispersion = ", scoreDispersion4)
}

main3(1000)

main3(1)
main3(2)
```

On peut remarquer que la 3ème hypothèse a un biais faible plus on augmente la taille du sample. La méthode 1 est moins précise que la 3 plus la taille du sample est elevée. 
Quand on a un petit nombre de sample on utilise la 1.
Quand on a un grand nombre de sample on utilise la 3. 

```{r}
print(Fx(0))
```




